import yfinance as yf
import pandas as pd
import numpy as np
import talib
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import xgboost as xgb

import alpaca_trade_api as tradeapi
import pandas_ta as ta
import matplotlib.pyplot as plt
import backtrader as bt
import re
import itertools

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler



#data = yf.download("AAPL", interval="1D", period='5Y')
data=pd.read_csv(r"D:\Python_Programming\Alpaca_trading\Version3.0\1D_3Y_AAPL.csv")

data.reset_index(inplace=True)
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)
print(data.shape)
data.drop(columns=['long_14_3','short_14_3','long_100_2','short_100_2','SMA_200','ema_200'],inplace=True)
data = data.iloc[100:]
#data.dropna(inplace=True)
data.fillna(0,inplace=True)
print(data.shape)

#%%
# Calculate features
'''
data['SMA_50'] = talib.SMA(data['Close'], timeperiod=50)
data['SMA_200'] = talib.SMA(data['Close'], timeperiod=200)
data['RSI'] = talib.RSI(data['Close'], timeperiod=14)
macd, signal, _ = talib.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)
data['MACD'] = macd
data['Signal'] = signal
'''
###############################################################################

'''
data[['SUPERT_14_3', 'direction_14_3', 'long_14_3', 'short_14_3']] = ta.supertrend(data['High'], data['Low'],data['Close'], length=14, multiplier=3)
data[['SUPERT_100_2', 'direction_100_2', 'long_100_2', 'short_100_2']] = ta.supertrend(data['High'], data['Low'], data['Close'], length=100, multiplier=2)

data.drop(columns=['long_100_2','short_100_2','long_14_3','short_14_3'],inplace=True)

timePeriods=[2,11,14]#,8,21
for i in timePeriods:
    data[f'RSI_{i}']=talib.RSI(data['Close'], i)
            
periods=[10,20,50]#5,100,,200
for i in periods:
    data[f'SMA_{i}']=data['Close'].rolling(window=i).mean()
    data[f'ema_{i}']=talib.EMA(data['Close'], timeperiod=i)
          
stock_periods=[[5,3,3],[6,3,3],[13,8,8],[15,3,3]]
        
for f,s,si in stock_periods:
    data[f'stock_k_{f}_{s}_{si}'], _ = talib.STOCH(data['High'], data['Low'], data['Close'], fastk_period=f, slowk_period=s, slowd_period=si)
        
stock_periods=[[12,26,9],[24,52,9],[19,39,9]]
for f,s,si in stock_periods:
    data[f'MACD_{f}_{s}_{si}'], data[f'Signal_{f}_{s}_{si}'], _ = talib.MACD(data['Close'], fastperiod=f, slowperiod=s, signalperiod=si)
            
            
stock_periods=[[20,20],[40,50],[100,100]]            
for l,u in stock_periods:
    data[[f'dcl_{l}_{u}',f'dcm_{l}_{u}',f'dcu_{l}_{u}']]=data.ta.donchian(lower_length = l, upper_length = u)            
            
    
data['Prev_Close'] = data['Close'].shift(1)
data['OBV'] = 0
data.loc[data['Close'] > data['Prev_Close'], 'OBV'] = data['Volume']
data.loc[data['Close'] < data['Prev_Close'], 'OBV'] = -data['Volume']
data['OBV'] = data['OBV'].cumsum()

###############################################################################
# Drop NaN values generated by TA-Lib
data.dropna(inplace=True)



# Create target variable: 1 if tomorrow's close price > today's close price, else 0
data['change1'] = (data['Close'].shift(-1) > data['Close']).astype(int)
data['change3'] = (data['Close'].shift(-3) > data['Close']).astype(int)
'''



f = (data['Close'].shift(-1) > data['Close']).astype(int)

# Define features and target variable
data.drop(columns=['Open','High','Low','Close','Adj Close','Volume','index','Datetime'],inplace=True)
features =data.columns.tolist()
data['Target']=f

data.to_csv("Rapid.csv")

print(features)
#  ['SUPERT_14_3', 'direction_14_3', 'SUPERT_100_2', 'direction_100_2',
#        'RSI_2', 'RSI_11', 'RSI_14', 'SMA_10', 'ema_10', 'SMA_20', 'ema_20',
#        'SMA_50', 'ema_50', 'stock_k_5_3_3', 'stock_k_6_3_3', 'stock_k_13_8_8',
#        'stock_k_15_3_3', 'MACD_12_26_9', 'Signal_12_26_9', 'MACD_24_52_9',
#        'Signal_24_52_9', 'MACD_19_39_9', 'Signal_19_39_9', 'dcl_20_20',
#        'dcm_20_20', 'dcu_20_20', 'dcl_40_50', 'dcm_40_50', 'dcu_40_50',
#        'dcl_100_100', 'dcm_100_100', 'dcu_100_100', 'Prev_Close', 'OBV','change1','change3']


target = 'Target'

#for feature in features:
#     data[f'{feature}_diff']=data[feature].diff()

data.dropna(inplace=True)

#%%
#correlation_matrix = data.corr()
# Generate heatmap
#plt.figure(figsize=(15, 10))
#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
#plt.title('Correlation Matrix')
#plt.show()



scaler = StandardScaler()
data[features] = scaler.fit_transform(data[features])

X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, shuffle=False)

model = Sequential([
    Dense(256, input_shape=(len(features),), activation='relu'),
    Dense(128, activation='relu'),
    #Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

# Predict on the testing set
test_predictions = (model.predict(X_test) > 0.5).astype(int).flatten()

accuracy = accuracy_score(y_test, test_predictions)
print(f"Simple NN-----Model Accuracy: {accuracy * 100:.2f}%")
# '''
# # Simulate real-time predictions
# correct_hits = 0
# for i in range(len(X_test)):
#     next_day_features = X_test.iloc[i].values.reshape(1, -1)
#     next_day_pred = (model.predict(next_day_features) > 0.5).astype(int)[0][0]
    
#     if next_day_pred == y_test.iloc[i]:
#         correct_hits += 1
        
#         # Create a new DataFrame for the new data point
#         new_data_point = pd.DataFrame([X_test.iloc[i].values.tolist() + [y_test.iloc[i]]], columns=features + [target])
        
#         # Append the new data point to the training data
#         X_train = pd.concat([X_train, new_data_point[features]], ignore_index=True)
#         y_train = pd.concat([y_train, new_data_point[target]], ignore_index=True)

# # Output the number of correct hits and the updated training data size
# print(f"Number of Correct Hits: {correct_hits/len(X_test)}")
# print(f"Updated Training Data Size: {len(X_train)}")
# '''

# Split the dataset using splitdatavalidation


# Initialize and train the Random Forest Classifier
#clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)

clf.fit(X_train, y_train)

# Predict on the testing set
test_predictions = clf.predict(X_test)

accuracy = accuracy_score(y_test, test_predictions)
print(f"XGBOOST----Model Accuracy: {accuracy * 100:.2f}%")


# # Simulate real-time predictions
# correct_hits = 0
# for i in range(len(X_test)):
#     #clf.fit(X_train, y_train)
#     next_day_features = X_test.iloc[i].values.reshape(1, -1)
#     next_day_features_df = pd.DataFrame(next_day_features, columns=features)
#     next_day_pred = clf.predict(next_day_features_df)[0]
    
#     if next_day_pred == y_test.iloc[i]:
#         correct_hits += 1
        
#         # Create a new DataFrame for the new data point
#         new_data_point = pd.DataFrame([X_test.iloc[i].values.tolist() + [y_test.iloc[i]]], columns=features + [target])
        
#         # Append the new data point to the training data
#         X_train = pd.concat([X_train, new_data_point[features]], ignore_index=True)
#         y_train = pd.concat([y_train, new_data_point[target]], ignore_index=True)

# # Output the number of correct hits and the updated training data size
# print(f"Number of Correct Hits: {correct_hits/len(X_test)}")
# print(f"Updated Training Data Size: {len(X_train)}")













from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam


X_train_lstm = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test_lstm = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])

# Create a Sequential model
model = Sequential([
    LSTM(128, input_shape=(1, len(features)), activation='relu'),
    Dense(64, input_shape=(1, len(features)), activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Fit the model
model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)

# Test the model
test_loss, test_acc = model.evaluate(X_test_lstm, y_test)
print(f"LSTM---Test Accuracy: {test_acc * 100:.2f}%")




from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

# Reshape data for CNN (samples, features, channels)
X_train_cnn = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)

# Create a Sequential model
model = Sequential([
    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(len(features), 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Fit the model
model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, verbose=0)

# Test the model
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"CNN---Test Accuracy: {test_acc * 100:.2f}%")














